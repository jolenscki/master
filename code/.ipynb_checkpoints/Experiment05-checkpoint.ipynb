{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78faa31-2b19-49f1-9872-05c40cc323a8",
   "metadata": {},
   "source": [
    "# Experiment 04: Criterion Function\n",
    "\n",
    "Evaluating impact of the criterion function of the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b1a78a-7bec-46ea-84f3-26782ab09084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path as osp\n",
    "# if 'jupyter' in os.getcwd():\n",
    "#     os.chdir(osp.join(os.getcwd(), 'masterarbeit', 'code'))\n",
    "import glob\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdmnotebook\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from typing import Callable\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from itertools import cycle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "mpl.rc('axes', unicode_minus=False)\n",
    "preamble = r'\\usepackage{amsmath}'  # LaTeX preamble command\n",
    "mpl.rcParams['text.latex.preamble'] = preamble\n",
    "\n",
    "# import seaborn as sns\n",
    "import networkx as nx\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "from torch import Tensor, nn, cuda\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# pytorch geometric imports\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import Compose\n",
    "\n",
    "from torch_geometric_temporal import GConvLSTM\n",
    "\n",
    "# lightning imports\n",
    "from lightning.pytorch.utilities.combined_loader import CombinedLoader\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import sys\n",
    "# Add the 'code' directory to sys.path to make the  submodules available\n",
    "# sys.path.append('/home/jupyter/masterarbeit/code')\n",
    "\n",
    "from util.utils import generate_log_name\n",
    "from util.plot_utils import *\n",
    "\n",
    "import logging as log\n",
    "\n",
    "from data.dataset.GraphDataset import GraphDataset\n",
    "\n",
    "from model.transform import CollapseChannels, ExtractSquare\n",
    "from model.autoencoder import Autoencoder\n",
    "\n",
    "from model.criterions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9f23d2-f9de-42ef-8049-8015d230009b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_id = 'exp04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88132c3-aa06-4346-9298-283280e25611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_WORKERS: int = 0\n",
    "BATCH_SIZE: int = 32\n",
    "NUM_CHANNELS: int = 2\n",
    "WDW_LENGTH: list = [12, 6]\n",
    "\n",
    "# Constants that I may change a bit during testing\n",
    "tgt: str = 'MELBOURNE'\n",
    "src_list: list = ['ANTWERP', 'BANGKOK']\n",
    "# src_list: list = ['ANTWERP', 'BANGKOK', 'BARCELONA', 'BERLIN', 'CHICAGO', 'ISTANBUL', 'MOSCOW'] # 7 cities\n",
    "EPOCHS_OFFLINE: int = 2\n",
    "tgt_data_limit: int = 1680\n",
    "src_data_limit: int = None\n",
    "LOGGING: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e7122a-8e66-41b8-b117-cd8b1b4dc088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if src_data_limit == -1:\n",
    "    src_data_limit = None\n",
    "\n",
    "# Get data from bucket\n",
    "bucket_name = 'cloud-ai-platform-054ad037-69b6-4c4d-94a1-75d2591213c7'\n",
    "bucket_folder = 'data/graphs'\n",
    "local_folder  = 'data/graphs'\n",
    "download_directory(bucket_name, bucket_folder, local_folder)\n",
    "bucket_folder = 'data/raw'\n",
    "local_folder  = 'data/raw'\n",
    "download_directory(bucket_name, bucket_folder, local_folder)\n",
    "\n",
    "bucket_output = 'output/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b1fbc7-14f3-4942-98a7-129b74e2a53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants that I don't intend to change much\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TRAIN_VAL_TEST_SPLIT = [0.8, 0.1, 0.1]\n",
    "\n",
    "pre_transform = Compose([\n",
    "    CollapseChannels(),\n",
    "    ExtractSquare(50, 'central'),\n",
    "])\n",
    "\n",
    "static_transform = Compose([\n",
    "    ExtractSquare(50, 'central'),\n",
    "])\n",
    "\n",
    "ds_kwargs = {\n",
    "    'root_dir': 'data/raw',\n",
    "    'device': device,\n",
    "    'pre_transform': pre_transform,\n",
    "    'static_transform': static_transform,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f85cb87-32d3-4f64-b06b-09a46793e98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seed generator for DataLoader\n",
    "torch.manual_seed(2311)\n",
    "\n",
    "# Create datasets for each city\n",
    "ds_dict = {}\n",
    "for city in src_list:\n",
    "    ds_dict[city] = GraphDataset(\n",
    "        cities=[city],\n",
    "        limit=src_data_limit,\n",
    "        **ds_kwargs,\n",
    "    )\n",
    "    \n",
    "ds_dict[tgt] = GraphDataset(\n",
    "    cities=[tgt],\n",
    "    limit=tgt_data_limit,\n",
    "    **ds_kwargs,\n",
    ")\n",
    "\n",
    "# Split each dataset into training and test sets\n",
    "train = {}\n",
    "val   = {}\n",
    "test  = {}\n",
    "for city in ds_dict:\n",
    "    train_ds, val_ds, test_ds = random_split(\n",
    "        ds_dict[city], TRAIN_VAL_TEST_SPLIT\n",
    "    )\n",
    "    train[city] = DataLoader(train_ds, batch_size=BATCH_SIZE,  shuffle=True)\n",
    "    val[city]   = DataLoader(  val_ds, batch_size=BATCH_SIZE,  shuffle=True)\n",
    "    test[city]  = DataLoader( test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Create dataloader for offline training with source cities\n",
    "source_train = {city: train[city] for city in src_list}\n",
    "source_dataloader = CombinedLoader(source_train, mode='max_size_cycle')\n",
    "\n",
    "source_test = {city: test[city] for city in src_list}\n",
    "sourcetest_dataloader = CombinedLoader(source_test, mode='max_size_cycle')\n",
    "\n",
    "# Create dataloader for online training with source and target cities\n",
    "train_dataloader = CombinedLoader(train, mode='max_size_cycle')\n",
    "\n",
    "# Create dataloader for validation with source and target cities\n",
    "val_dataloader = CombinedLoader(val, mode='max_size_cycle')\n",
    "\n",
    "# Create dataloader for testing with source and target cities\n",
    "test_dataloader = CombinedLoader(test, mode='max_size_cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c8fbae0-900d-4689-926e-9d99d6209d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AE_criterion   = WeightedMSELoss(weight=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ab68a-e0c9-4494-b2c7-7cb7c5c08e38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE_criterion=WeightedMSELoss()_activation='tanh'_conv_dim=16_num_channels=2_K_cheb=4_device=device(type='cuda')_dropout=0.5_linear_dim=8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c549f068e0244dfafb845411c107a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbdb88e0d2f4de19ebc377032fd76cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1081 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv_dim=16\n",
    "linear_dim=8\n",
    "num_channels=NUM_CHANNELS\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "activation='tanh'\n",
    "dropout=0.5\n",
    "K_cheb=4\n",
    "\n",
    "criterions = {\n",
    "    r'WMSE ($w=100$)': WeightedMSELoss(weight=100),\n",
    "    r'WMSE ($w=10$)': WeightedMSELoss(weight=10),\n",
    "    r'MSE': nn.MSELoss(),\n",
    "    r'WMSLE ($w=100$)': WeightedMSLELoss(weight=100),\n",
    "    r'WMSLE ($w=10$)': WeightedMSLELoss(weight=10),\n",
    "    r'MSLE': MSLELoss(),\n",
    "    r'Focal ($\\alpha=0.25, \\gamma=2$)': FocalLoss(alpha=0.25, gamma=2),\n",
    "    r'CustomHuber ($\\delta=0.1, w=100$)': CustomHuberLoss(delta=.1, zero_inflation_weight=0.01),\n",
    "    r'LogCosh': LogCoshLoss(),\n",
    "}\n",
    "\n",
    "test_losses_dict = {}\n",
    "for crit_name, AE_criterion in criterions.items():\n",
    "    specs = f'{crit_name=}_{activation=}_{conv_dim=}_{num_channels=}_{K_cheb=}_{device=}_{dropout=}_{linear_dim=}'\n",
    "    print(specs)\n",
    "    ae = Autoencoder(conv_dim=conv_dim,\n",
    "                     num_channels=num_channels,\n",
    "                     K_cheb=K_cheb,\n",
    "                     device=device,\n",
    "                     activation=activation,\n",
    "                     dropout=dropout,\n",
    "                     linear_dim=linear_dim,\n",
    "                     ).to(device)\n",
    "\n",
    "    lr = 5e-3\n",
    "    l2_decay = 5e-4\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': ae.parameters()},\n",
    "        ], lr=lr, weight_decay=l2_decay)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    tqdm_bar_fmt = '{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]'\n",
    "    len_train = source_dataloader._dataset_length()//BATCH_SIZE + 1\n",
    "\n",
    "    ae.train()\n",
    "    # fn = generate_log_name(prefix=\"autoencoder\", fdir=\"training logs\")\n",
    "    # log.basicConfig(filename=osp.join('training logs', fn), level=log.INFO)\n",
    "    # Initialize lists to store losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    # Outer progress bar for epochs\n",
    "    with tqdmnotebook(total=EPOCHS_OFFLINE,\n",
    "                      leave=True,\n",
    "                      # mininterval=20,\n",
    "                      desc='Epochs',\n",
    "                      colour='blue',\n",
    "                      bar_format=tqdm_bar_fmt) as pbar_epochs:\n",
    "        for epoch in range(EPOCHS_OFFLINE):\n",
    "            with tqdmnotebook(\n",
    "                total=len_train,\n",
    "                leave=True,\n",
    "                # mininterval=20,\n",
    "                desc='Batches',\n",
    "                colour='green',\n",
    "                bar_format=tqdm_bar_fmt) as pbar_batches:\n",
    "                for databatch, i, _ in source_dataloader:\n",
    "                    total_loss = 0\n",
    "                    for city, data in databatch.items():\n",
    "                        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            x_recons = ae(x, edge_index)\n",
    "                            x = x.reshape(x_recons.shape)\n",
    "                            loss = AE_criterion(x_recons, x)\n",
    "                        total_loss += loss\n",
    "                    # Scales loss and calls backward() to create scaled gradients\n",
    "                    scaler.scale(total_loss).backward(retain_graph=True)\n",
    "\n",
    "                    # Unscales gradients and calls optimizer.step()\n",
    "                    scaler.step(optimizer)\n",
    "\n",
    "                    # Updates the scale for next iteration\n",
    "                    scaler.update()\n",
    "                    train_losses.append(total_loss.item())\n",
    "                    # Zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    pbar_batches.update(1)\n",
    "            # reconstruction_plot(x, x_recons, specs=specs, save=True, show=True, exp_id=exp_id)\n",
    "            pbar_epochs.update(1)\n",
    "\n",
    "    # train_losses_dict[crit_name] = train_losses\n",
    "\n",
    "    # reconstruction_plot(x, x_recons, specs=specs, save=False, show=True)\n",
    "    folder = osp.join('training logs', 'models', exp_id)\n",
    "    check_dir(folder)\n",
    "    torch.save(ae.state_dict(), osp.join(folder, f'{crit_name}.pth'))\n",
    "        # After all epochs\n",
    "    ae.eval()\n",
    "    # total_test_loss = 0\n",
    "    # test_losses = []\n",
    "    errors = {\n",
    "        'MAE': torch.empty(0, device=device, dtype=torch.float16), \n",
    "        'MSE': torch.empty(0, device=device, dtype=torch.float16)\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        for databatch, i, _ in sourcetest_dataloader:\n",
    "            for city, data in databatch.items():\n",
    "                batch_size = int(data.ptr.shape[0] - 1)\n",
    "                x, edge_index = data.x, data.edge_index\n",
    "                x_recons = ae(x, edge_index)\n",
    "                \n",
    "                x = x.reshape(batch_size, -1)\n",
    "                x_recons = x_recons.reshape(batch_size, -1)\n",
    "                \n",
    "                mae = torch.mean(torch.abs(x - x_recons), dim=1)\n",
    "                mse = torch.mean(torch.square(x - x_recons), dim=1)\n",
    "                \n",
    "                errors['MAE'] = torch.cat([errors['MAE'], mae], dim=0)\n",
    "                errors['MSE'] = torch.cat([errors['MSE'], mse], dim=0)\n",
    "    \n",
    "    test_losses_dict[crit_name] = errors\n",
    "    plot_losses_boxplot(test_losses_dict, variable=r'$Criterion Function$', \n",
    "                        errors=['MAE', 'MSE'], specs=\"\", save=False, show=True,\n",
    "                        exp_id=exp_id)\n",
    "    \n",
    "plot_losses_boxplot(test_losses_dict, variable=r'$Criterion Function$', \n",
    "                    errors=['MAE', 'MSE'], specs=specs, save=True, show=True,\n",
    "                    exp_id=exp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29593c33-432c-4fb5-8fdb-2cb433a51d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
